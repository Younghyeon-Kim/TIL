# Feature Scaling

`전처리과정`에서 아주 중요한 `표준화`, `정규화`에 대해 살펴보자 

- `확률 분포`에 대한 개념이 있어야 `표준화`를 이해하기 쉽다.

`이상치 제거`는 `데이터 분석과정`에서 정말 중요



### 0. feature scaling

- 정의

  입력된 데이터에는 각각의 `feature`가 있다.

  해당 `feature`들의 **값을 일정한 수준으로 맞춰주는 것** = `Feature scaling` 이라 불림

- 스케일링 방법

  `표준화(standardization)`, `정규화(normalization)`

  

  **Q. feature scaling을 하는 이유?**

  ex)

  값의 평균이 음수이고 대다수의 값이 음수,

  반면에 `column`값의 평균이 양수이고 대다수의 값 양수이다.

  - 두 `feature`의 값들을 그대로 넣었을 때

    각각의 `평균`과 `분산`, `최대 최소 값`이 제각각이기 때문에

    두 `feature`를 비교하기 어렵고 따라서 `학습 성능이 떨어짐`


  - `표준화`를 통해 두 피쳐들의 값들을 평균 0 표편1 인 표준 정규 분포로 만들어 주거나

    `정규화` 과정을 통해서 값의 범위를 0~1 로 제한해주면 학습 성능이 떨어지는 것을 막을 수 있다.

    :star:**모델의 학습 성능을 높이기 위해서 `feature scaling`을 하는 것이다.** 

  

### 1. 표준화 (standardization)

- **입력된 `x` 값들의 정규 분포를 `평균 = 0` 이고 `분산 = 1` 인 `표준 정규 분포`로 변환** 

​		해당 `feature`가 연속된 값이기 때문에 `평균`과 `분산`을 구할 수 있다. 

​		이를 통해 정규 분포를 그릴 수 있는데, `표준화`를 하면

​		해당 정규 분포를 평균이 0이고 분산이 1 인 `표준 정규 분포`로 바꿔 준다. 



### 2. 정규화 (normalization)

- **입력된 `x`값들을 모두 `0과 1사이`의 값으로 변환** 

​		각각 `feature` 값의 범위가 크게 차이가 날 때 

​		`정규화`를 시켜준다면, 모두 0~1 사이의 값으로 `비교하기 용이`해진다. 