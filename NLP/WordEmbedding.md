# 단어 수준의 임베딩

## 1. NPLM

`Neural Probabilistic Language Model`

: **뉴럴 네트워크**를 사용하는 **확률론(통계)적**인 언어 모델

- 자연어 처리 분야에 `Embedding` 개념을 널리 퍼뜨리는 데 일조한 선구자적 모델



### 의의

전통적인 언어 모델의 한계를 극복하는 과정에서 탄생

- 기존 언어 모델의 단점

  1. 학습 데이터에 존재하지 않는 n-gram이 포함된 문장이 나타날 **확률 값을 `0`**으로 부여한다.

     물론 **Back-off**나 **Smooting**으로 이런 문제를 일부 보완할 수 있지만 완전한 것은 아니다.

  2. 문장의 **장기 의존성(Long-term dependency)을 포착해내기 어렵다.**

     다시 말해 n-gram 모델의 **n을 5 이상으로 길게 설정할 수 없다.**

     **n이 커질수록 그 등장 확률이 0인 단어 시퀀스가 기하급수적으로 늘어난다.**

  3. **단어/문장 간 유사도를 계산할 수 없다.**



### 학습

단어 시퀀스가 주어졌을 떄 다음 단어가 무엇인지 맞추는 과정에서 학습된다.

직전까지 등장한 `n-1`의 단어들로 다음 단어를 맞추는 **n-gram 언어 모델**이다.



## 2. Word2vec

### 1. CBOW

### 2. Skip-Gram



## 3. FastText



## 4. Glove



## 5. Swivel

